{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "158a3d4c-0023-4790-888b-d692a417d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F  \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "effc5747-0e84-4f71-895c-066414353d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.1       # Max perturbation (for L∞ PGD)\n",
    "ALPHA = 0.01         # Step size per iteration\n",
    "ATTACK_ITERATIONS = 40\n",
    "TARGET_LABEL = 1     # Example target label for the targeted attack\n",
    "\n",
    "# System/Model parameters\n",
    "sys.path.append(\"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/models\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/weights/best_model_retrained.pth\"\n",
    "#IQ_FILE_PATH = \"/home/jfeng/Desktop/jfeng/Pluto_10/Pluto_10_2m_run3.iq\"\n",
    "\n",
    "IQ_FILE_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/1m_2m_replacedPluto4/1m_2m/Pluto_0_1m_run3.iq\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce6f54b6-9f60-40fb-bfcc-f99ffb26a896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model weights from: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/weights/best_model_retrained.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3017044/2189494409.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet1D(\n",
       "  (conv1): Conv1d(2, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck1D(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck1D(\n",
       "      (conv1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Bottleneck1D(\n",
       "      (conv1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck1D(\n",
       "      (conv1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck1D(\n",
       "      (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Bottleneck1D(\n",
       "      (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Bottleneck1D(\n",
       "      (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck1D(\n",
       "      (conv1): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(1024, 2048, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck1D(\n",
       "      (conv1): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Bottleneck1D(\n",
       "      (conv1): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=2048, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from attempt2 import resnet50_1d  # Directly import from attempt2.py\n",
    "num_classes = 8  # Change this if your model was trained with a different number of classes\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = resnet50_1d(num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "# Load trained weights\n",
    "print(f\"Loading trained model weights from: {MODEL_PATH}\")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "244ca700-ca37-452a-be48-0392caac522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        sample = torch.from_numpy(sample).float()\n",
    "        # Normalize data\n",
    "        magnitude = torch.sqrt(torch.sum(sample**2, dim=1, keepdim=True))\n",
    "        sample = sample / magnitude\n",
    "\n",
    "        label_tensors = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return sample, label_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e65c33d4-7d72-4113-b0da-6f3c2d51b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3017044/3472265293.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/1m_2m_replacedPluto4/1m_2m/Pluto_1_2m_run3.iq\n",
      "[Pluto_1_2m_run3.iq] Accuracy: 95.75% | Mismatches: 51 / 1200\n"
     ]
    }
   ],
   "source": [
    "TRUE_IQ_FILE_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/1m_2m_replacedPluto4/1m_2m/Pluto_1_2m_run3.iq\"\n",
    "BATCH_SIZE = 16\n",
    "WINDOW_SIZE = 10000\n",
    "HOP_SIZE = 500\n",
    "START_INDEX = 4800\n",
    "END_INDEX = 6000\n",
    "\n",
    "# Initialize and load model\n",
    "model = resnet50_1d(num_classes=8)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "fname = os.path.basename(TRUE_IQ_FILE_PATH)\n",
    "print(f\"\\nProcessing file: {TRUE_IQ_FILE_PATH}\")\n",
    "\n",
    "# Extract label from filename: Pluto_#_...\n",
    "try:\n",
    "    target_label = int(fname.split(\"_\")[1])\n",
    "except (IndexError, ValueError):\n",
    "    print(f\"  Warning: could not parse label from filename: {fname}\")\n",
    "    \n",
    "\n",
    "# Load IQ data\n",
    "data = np.fromfile(TRUE_IQ_FILE_PATH, dtype=\"float32\")\n",
    "real_part = data[0::2]\n",
    "imag_part = data[1::2]\n",
    "\n",
    "test_data_tensors = []\n",
    "test_label_tensors = []\n",
    "\n",
    "# Sliding window generation\n",
    "for x in range(START_INDEX, END_INDEX):\n",
    "    start = (x + 1) * HOP_SIZE\n",
    "    end = start + WINDOW_SIZE\n",
    "    if end > len(real_part): break\n",
    "\n",
    "    i_window = real_part[start:end]\n",
    "    q_window = imag_part[start:end]\n",
    "    combined = np.vstack((i_window, q_window))  # [2, WINDOW_SIZE]\n",
    "    test_data_tensors.append(combined)\n",
    "    test_label_tensors.append(target_label)\n",
    "\n",
    "if not test_data_tensors:\n",
    "    print(f\"Skipping {fname}: not enough valid IQ segments.\")\n",
    "    \n",
    "\n",
    "# Stack and shuffle\n",
    "test_data_tensors = np.stack(test_data_tensors, axis=0)\n",
    "test_label_tensors = np.array(test_label_tensors)\n",
    "indices = np.random.permutation(len(test_data_tensors))\n",
    "test_data = test_data_tensors[indices]\n",
    "test_labels = test_label_tensors[indices]\n",
    "\n",
    "# Create Dataset + Loader\n",
    "test_dataset = IQDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Evaluate\n",
    "correct = 0\n",
    "total = 0\n",
    "mismatch_count = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        mismatch_count += (predicted != labels).sum().item()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "acc = correct / total * 100\n",
    "print(f\"[{fname}] Accuracy: {acc:.2f}% | Mismatches: {mismatch_count} / {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "827b8886-956f-435c-8afb-dd736bfbf2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targeted_pgd_attack(model, x, y, target_label, eps, alpha, num_iter):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(target_label, int):\n",
    "        target_label = torch.full_like(y, target_label)\n",
    "    \n",
    "    x_adv = x.clone().detach().to(DEVICE)\n",
    "    x_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(x_adv)\n",
    "        \n",
    "        loss = -nn.CrossEntropyLoss()(outputs, target_label)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        grad = x_adv.grad.data\n",
    "        x_adv = x_adv.detach() + alpha * grad.sign()\n",
    "\n",
    "        x_adv = torch.min(torch.max(x_adv, x - eps), x + eps)\n",
    "\n",
    "        x_adv.requires_grad = True\n",
    "\n",
    "    return x_adv.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4105454a-4988-4e49-9459-5bff59047397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/1m_2m_replacedPluto4/1m_2m/Pluto_0_1m_run3.iq\n",
      "True label: 0, Target label: 1\n",
      "Original prediction: 0, Confidence: 0.9982\n",
      "Adversarial prediction: 1, Confidence: 1.0000\n",
      "Saved perturbation to Apr18/pluto0_to_target1_noise.iq\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(f\"Loading data from: {IQ_FILE_PATH}\")\n",
    "\n",
    "    # Use true label from filename (Pluto_0 → class 0)\n",
    "    label = int(os.path.basename(IQ_FILE_PATH).split(\"_\")[1])  # This will be 0\n",
    "    print(f\"True label: {label}, Target label: {TARGET_LABEL}\")\n",
    "\n",
    "    # Load and format IQ data\n",
    "    data = np.fromfile(IQ_FILE_PATH, dtype=\"float32\")\n",
    "    real = data[0::2]\n",
    "    imag = data[1::2]\n",
    "\n",
    "    start = (START_INDEX + 1) * HOP_SIZE\n",
    "    end = start + WINDOW_SIZE\n",
    "    i_window = real[start:end]\n",
    "    q_window = imag[start:end]\n",
    "    combined = np.vstack((i_window, q_window))  # [2, N]\n",
    "\n",
    "    # Wrap in Dataset for normalization\n",
    "    test_dataset = IQDataset([combined], [label])\n",
    "    data_tensor, label_tensor = test_dataset[0]\n",
    "    data_tensor = data_tensor.unsqueeze(0).to(DEVICE)\n",
    "    label_tensor = label_tensor.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Prediction before attack\n",
    "    with torch.no_grad():\n",
    "        logits = model(data_tensor)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        orig_pred = torch.argmax(probs, dim=1)\n",
    "        orig_conf = probs.max(dim=1).values\n",
    "\n",
    "    print(f\"Original prediction: {orig_pred.item()}, Confidence: {orig_conf.item():.4f}\")\n",
    "\n",
    "    # PGD attack targeting label 1\n",
    "    x_adv = targeted_pgd_attack(\n",
    "        model=model,\n",
    "        x=data_tensor,\n",
    "        y=label_tensor,\n",
    "        target_label=TARGET_LABEL,\n",
    "        eps=EPSILON,\n",
    "        alpha=ALPHA,\n",
    "        num_iter=ATTACK_ITERATIONS\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_adv = model(x_adv)\n",
    "        probs_adv = F.softmax(logits_adv, dim=1)\n",
    "        adv_pred = torch.argmax(probs_adv, dim=1)\n",
    "        adv_conf = probs_adv.max(dim=1).values\n",
    "\n",
    "    print(f\"Adversarial prediction: {adv_pred.item()}, Confidence: {adv_conf.item():.4f}\")\n",
    "\n",
    "    # Save perturbation\n",
    "    original_np = data_tensor.squeeze().cpu().numpy()\n",
    "    adv_np = x_adv.squeeze().cpu().numpy()\n",
    "    I_diff = adv_np[0] - original_np[0]\n",
    "    Q_diff = adv_np[1] - original_np[1]\n",
    "\n",
    "    interleaved = np.empty(I_diff.size + Q_diff.size, dtype=np.float32)\n",
    "    interleaved[0::2] = I_diff\n",
    "    interleaved[1::2] = Q_diff\n",
    "\n",
    "    save_path = \"Apr18/pluto0_to_target1_noise.iq\"\n",
    "    interleaved.tofile(save_path)\n",
    "    print(f\"Saved perturbation to {save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d351bb0-befe-4838-b7f2-d21a6416dcce",
   "metadata": {},
   "source": [
    "Test the newly generated sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2729b314-6d77-4873-a3e2-c9574b6ee33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing saved perturbation from: Apr18/pluto0_to_target1_noise.iq\n",
      "Prediction on saved noise sample: 1, Confidence: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def test_saved_noise_sample():\n",
    "    save_path = \"Apr18/pluto0_to_target1_noise.iq\"\n",
    "    print(f\"\\nTesting saved perturbation from: {save_path}\")\n",
    "\n",
    "    # Recompute label from filename\n",
    "    label = int(os.path.basename(IQ_FILE_PATH).split(\"_\")[1])  # e.g., Pluto_0 → 0\n",
    "\n",
    "    # Load the original IQ data again\n",
    "    data = np.fromfile(IQ_FILE_PATH, dtype=\"float32\")\n",
    "    real = data[0::2]\n",
    "    imag = data[1::2]\n",
    "\n",
    "    start = (START_INDEX + 1) * HOP_SIZE\n",
    "    end = start + WINDOW_SIZE\n",
    "    i_window = real[start:end]\n",
    "    q_window = imag[start:end]\n",
    "\n",
    "    # Load the saved noise\n",
    "    noise = np.fromfile(save_path, dtype=\"float32\")\n",
    "    I_diff = noise[0::2]\n",
    "    Q_diff = noise[1::2]\n",
    "\n",
    "    # Apply the perturbation\n",
    "    perturbed = np.vstack((i_window + I_diff, q_window + Q_diff))  # Shape [2, N]\n",
    "\n",
    "    # Wrap in dataset and tensor\n",
    "    test_dataset = IQDataset([perturbed], [label])\n",
    "    data_tensor, _ = test_dataset[0]\n",
    "    data_tensor = data_tensor.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Run through the model\n",
    "    with torch.no_grad():\n",
    "        logits = model(data_tensor)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1)\n",
    "        conf = probs.max(dim=1).values\n",
    "\n",
    "    print(f\"Prediction on saved noise sample: {pred.item()}, Confidence: {conf.item():.4f}\")\n",
    "\n",
    "test_saved_noise_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b150585-5ceb-4f5e-baee-534e352cd649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_channel_effects(x, min_distance=1.0, max_distance=2.0, path_loss_exponent=2.0, reference_distance=1.0, noise_std=0.01, apply_fading=True, apply_phase=True):\n",
    "    # Sample a random distance between min and max\n",
    "    d = torch.empty(1, device=x.device).uniform_(min_distance, max_distance)\n",
    "\n",
    "    # Path loss scaling\n",
    "    scaling = (reference_distance / d) ** (path_loss_exponent/2)\n",
    "    x_transformed = x * scaling\n",
    "\n",
    "    # # Apply fading (Rayleigh fading)\n",
    "    # if apply_fading:\n",
    "    #     fading = torch.sqrt(torch.randn_like(x_transformed)**2 + torch.randn_like(x_transformed)**2)\n",
    "    #     x_transformed = x_transformed * fading\n",
    "\n",
    "    # # Add noise\n",
    "    # noise = torch.randn_like(x_transformed) * noise_std\n",
    "    # x_transformed = x_transformed + noise\n",
    "\n",
    "    # # Apply phase rotation (approximate for I/Q representation)\n",
    "    # if apply_phase:\n",
    "    #     phase = torch.empty(1, device=x.device).uniform_(0, 2 * np.pi)\n",
    "    #     x_transformed = x_transformed * torch.cos(phase)  # Simplified phase shift\n",
    "\n",
    "    return x_transformed\n",
    "\n",
    "def apply_rayleigh_fading_amplitude(signal):\n",
    "    \"\"\"\n",
    "    Apply Rayleigh fading to the signal, which is of shape [2, N] (I and Q).\n",
    "    Fading is applied independently to the I and Q components.\n",
    "    \"\"\"\n",
    "    i_window = signal[0]\n",
    "    q_window = signal[1]\n",
    "\n",
    "    # Generate Rayleigh fading coefficients (real and imaginary parts)\n",
    "    real_fade = torch.randn(i_window.shape) / torch.sqrt(torch.tensor(2.0))\n",
    "    imag_fade = torch.randn(i_window.shape) / torch.sqrt(torch.tensor(2.0))\n",
    "\n",
    "    # Apply Rayleigh fading to the signal\n",
    "    faded_real = i_window * real_fade - q_window * imag_fade\n",
    "    faded_imag = i_window * imag_fade + q_window * real_fade\n",
    "\n",
    "    # Reconstruct the faded signal back into a tensor\n",
    "    faded_signal = torch.stack((faded_real, faded_imag))\n",
    "\n",
    "    return faded_signal\n",
    "\n",
    "def add_awgn_with_complex_std(signal, noise_std):\n",
    "    \"\"\"\n",
    "    Add complex AWGN noise to the signal of shape [2, N] (I and Q).\n",
    "    noise_std is the total standard deviation of the complex noise.\n",
    "    \"\"\"\n",
    "    # Split noise equally between I and Q (to maintain total variance)\n",
    "    per_dim_std = noise_std / math.sqrt(2)\n",
    "    #print(\"Per dim std: \", per_dim_std)\n",
    "    # Generate i.i.d. Gaussian noise for I and Q\n",
    "    noise = torch.randn_like(signal) * per_dim_std\n",
    "    #print(\"Noise: \", noise)\n",
    "    # Add noise to the signal\n",
    "    noisy_signal = signal + noise\n",
    "\n",
    "    return noisy_signal\n",
    "\n",
    "\n",
    "def apply_rayleigh_fading_and_awgn_amplitude(signal, noise_std):\n",
    "    \"\"\"\n",
    "    Apply Rayleigh fading (amplitude-based) and then add complex AWGN noise to the signal.\n",
    "    signal is of shape [2, N] (I and Q).\n",
    "    noise_std is the total standard deviation of the complex noise.\n",
    "    \"\"\"\n",
    "    print(signal)\n",
    "    # Apply Rayleigh fading (amplitude-based)\n",
    "    faded_signal = apply_rayleigh_fading_amplitude(signal)\n",
    "    #faded_signal = test_rayleigh(signal)\n",
    "    print(faded_signal)\n",
    "    # Add AWGN noise\n",
    "    noisy_signal = add_awgn_with_complex_std(faded_signal, noise_std)\n",
    "    #noisy_signal = test_awgn(faded_signal, noise_std)\n",
    "    print(noisy_signal)\n",
    "    return noisy_signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbeb192-ab23-4067-9a2c-357c4594840c",
   "metadata": {},
   "source": [
    "Create PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d97d619-6d3b-40e1-8b2a-4bc2b89511f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/1m_2m_replacedPluto4/1m_2m/Pluto_0_1m_run3.iq\n",
      "True label: 0, Target label: 1\n",
      "Saved EoT PGD noise to Apr18/mid_dist_eot_pluto10_to_target1_noise.iq\n",
      "\n",
      "Testing saved EoT noise from: Apr18/mid_dist_eot_pluto10_to_target1_noise.iq\n",
      "Full softmax probabilities:\n",
      "[[9.992830e-01 7.170069e-04 0.000000e+00 5.786715e-09 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]]\n",
      "Prediction on saved noise sample: 0, Confidence: 0.9993\n"
     ]
    }
   ],
   "source": [
    "def targeted_eot_pgd_attack(\n",
    "    model,\n",
    "    x,\n",
    "    y,\n",
    "    target_label,\n",
    "    eps=0.1,\n",
    "    alpha=0.01,\n",
    "    num_iter=80,\n",
    "    num_samples=10,\n",
    "    min_distance=1.0,\n",
    "    max_distance=2.0,\n",
    "    path_loss_exponent=2.0,\n",
    "    reference_distance=1.0,\n",
    "    noise_std=0.01,\n",
    "    apply_fading=True,\n",
    "    apply_phase=True\n",
    "):\n",
    "    x_adv = x.clone().detach().requires_grad_(True).to(DEVICE)\n",
    "    target = torch.full_like(y, target_label)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        total_grad = torch.zeros_like(x_adv)\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            x_t = transform_channel_effects(             #either add inverse or normal fading\n",
    "                x_adv,\n",
    "                min_distance=min_distance,\n",
    "                max_distance=max_distance,\n",
    "                path_loss_exponent=path_loss_exponent,\n",
    "                reference_distance=reference_distance,\n",
    "                noise_std=noise_std,\n",
    "                apply_fading=apply_fading,\n",
    "                apply_phase=apply_phase\n",
    "            )\n",
    "\n",
    "            logits = model(x_t)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "\n",
    "            grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "            total_grad += grad\n",
    "\n",
    "        avg_grad = total_grad / num_samples\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_adv -= alpha * avg_grad.sign()\n",
    "            x_adv = torch.max(torch.min(x_adv, x + eps), x - eps)\n",
    "            x_adv = x_adv.detach().clone().requires_grad_(True)\n",
    "\n",
    "    return x_adv\n",
    "\n",
    "def generate_eot_pgd_noise():\n",
    "    print(f\"Loading data from: {IQ_FILE_PATH}\")\n",
    "    label = int(os.path.basename(IQ_FILE_PATH).split(\"_\")[1])\n",
    "    print(f\"True label: {label}, Target label: {TARGET_LABEL}\")\n",
    "\n",
    "    # Load IQ data\n",
    "    data = np.fromfile(IQ_FILE_PATH, dtype=\"float32\")\n",
    "    real = data[0::2]\n",
    "    imag = data[1::2]\n",
    "    start = (START_INDEX + 1) * HOP_SIZE\n",
    "    end = start + WINDOW_SIZE\n",
    "    i_window = real[start:end]\n",
    "    q_window = imag[start:end]\n",
    "    combined = np.vstack((i_window, q_window))  # [2, N]\n",
    "\n",
    "    # Format into dataset\n",
    "    test_dataset = IQDataset([combined], [label])\n",
    "    data_tensor, label_tensor = test_dataset[0]\n",
    "    data_tensor = data_tensor.unsqueeze(0).to(DEVICE)\n",
    "    label_tensor = label_tensor.unsqueeze(0).to(DEVICE)\n",
    "    min_distance=1.0\n",
    "    max_distance=2.0\n",
    "    EPSILON = 0.1\n",
    "    ALPHA = 0.02\n",
    "    ATTACK_ITERATIONS = 40\n",
    "    if min_distance >= 5.0:\n",
    "        EPSILON = 0.6\n",
    "        ALPHA = 0.021\n",
    "        ATTACK_ITERATIONS = 100\n",
    "\n",
    "    # EoT PGD attack\n",
    "    #x_adv = data_tensor.clone().detach().to(DEVICE)\n",
    "\n",
    "    x_adv = targeted_eot_pgd_attack(\n",
    "        model=model,\n",
    "        x=data_tensor,\n",
    "        y=label_tensor,\n",
    "        target_label=TARGET_LABEL,\n",
    "        eps=EPSILON,\n",
    "        alpha=ALPHA,\n",
    "        num_iter=ATTACK_ITERATIONS,\n",
    "        num_samples=10,\n",
    "        min_distance=min_distance,\n",
    "        max_distance=max_distance,\n",
    "        noise_std=0.01\n",
    "    )\n",
    "\n",
    "    # Save perturbation only\n",
    "    original_np = data_tensor.squeeze().cpu().numpy()\n",
    "    adv_np = x_adv.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    I_diff = adv_np[0] - original_np[0]\n",
    "    Q_diff = adv_np[1] - original_np[1]\n",
    "\n",
    "    interleaved = np.empty(I_diff.size + Q_diff.size, dtype=np.float32)\n",
    "    interleaved[0::2] = I_diff\n",
    "    interleaved[1::2] = Q_diff\n",
    "\n",
    "    save_path = \"Apr18/mid_dist_eot_pluto10_to_target1_noise.iq\"\n",
    "    interleaved.tofile(save_path)\n",
    "    print(f\"Saved EoT PGD noise to {save_path}\")\n",
    "\n",
    "    # --- Evaluate by re-applying noise to original signal ---\n",
    "    print(f\"\\nTesting saved EoT noise from: {save_path}\")\n",
    "    noise = np.fromfile(save_path, dtype=np.float32)\n",
    "    I_noise = noise[0::2]\n",
    "    Q_noise = noise[1::2]\n",
    "\n",
    "    perturbed = np.vstack((i_window + I_noise, q_window + Q_noise))\n",
    "    test_dataset = IQDataset([perturbed], [label])\n",
    "    data_tensor_adv, _ = test_dataset[0]\n",
    "    data_tensor_adv = data_tensor_adv.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_adv = model(data_tensor_adv)\n",
    "        probs_adv = F.softmax(logits_adv, dim=1)\n",
    "        print(f\"Full softmax probabilities:\\n{probs_adv.cpu().numpy()}\")\n",
    "\n",
    "        adv_pred = torch.argmax(probs_adv, dim=1)\n",
    "        adv_conf = probs_adv.max(dim=1).values\n",
    "\n",
    "    print(f\"Prediction on saved noise sample: {adv_pred.item()}, Confidence: {adv_conf.item():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_eot_pgd_noise()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0650f31f-0db6-49e2-bf87-7aac21059820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: Apr16/mid_dist_eot_pluto0_to_target1_noise.iq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4087113/892844258.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Apr16/mid_dist_eot_pluto0_to_target1_noise.iq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m target_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# or whatever the expected true label is\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load IQ data\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRUE_IQ_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m real_part \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     21\u001b[0m imag_part \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Apr16/mid_dist_eot_pluto0_to_target1_noise.iq'"
     ]
    }
   ],
   "source": [
    "TRUE_IQ_FILE_PATH = \"Apr16/mid_dist_eot_pluto0_to_target1_noise.iq\"\n",
    "WINDOW_SIZE = 10000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model\n",
    "model = resnet50_1d(num_classes=8)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Get filename\n",
    "fname = os.path.basename(TRUE_IQ_FILE_PATH)\n",
    "print(f\"\\nProcessing file: {TRUE_IQ_FILE_PATH}\")\n",
    "\n",
    "# Manually assign label since filename doesn't follow Pluto_# format\n",
    "target_label = 0  # or whatever the expected true label is\n",
    "\n",
    "# Load IQ data\n",
    "data = np.fromfile(TRUE_IQ_FILE_PATH, dtype=\"float32\")\n",
    "real_part = data[0::2]\n",
    "imag_part = data[1::2]\n",
    "\n",
    "# Check file length\n",
    "start = 0\n",
    "end = start + WINDOW_SIZE\n",
    "if end > len(real_part):\n",
    "    raise ValueError(f\"Not enough samples in file: required {end}, got {len(real_part)}\")\n",
    "\n",
    "# Extract 1 window\n",
    "i_window = real_part[start:end]\n",
    "q_window = imag_part[start:end]\n",
    "combined = np.vstack((i_window, q_window))  # shape: [2, WINDOW_SIZE]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "test_dataset = IQDataset([combined], [target_label])\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1)\n",
    "        conf = probs.max(dim=1).values\n",
    "        print(f\"[{fname}] Prediction: {pred.item()}, Confidence: {conf.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14fbb63-ff96-498f-9108-596303855c21",
   "metadata": {},
   "source": [
    "TEST all the files in base dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975f6a7b-43b6-4227-8097-f36de55cefdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22101/3514134200.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto0/pluto0_2m_adv1m_concateotlow.iq\n",
      "[pluto0_2m_adv1m_concateotlow.iq] Prediction: 1, Confidence: 0.9998\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto0/pluto0_2m_adv2m_eotlow.iq\n",
      "[pluto0_2m_adv2m_eotlow.iq] Prediction: 0, Confidence: 0.9461\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto0/pluto0_2m_adv1m_concateotmid.iq\n",
      "[pluto0_2m_adv1m_concateotmid.iq] Prediction: 1, Confidence: 0.9994\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto0/pluto0_2m_adv1m_eotlow.iq\n",
      "[pluto0_2m_adv1m_eotlow.iq] Prediction: 4, Confidence: 0.9980\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto0/pluto0_2m_adv2m_noise.iq\n",
      "[pluto0_2m_adv2m_noise.iq] Prediction: 0, Confidence: 0.9961\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto0/pluto0_2m_adv1m_eotmid.iq\n",
      "[pluto0_2m_adv1m_eotmid.iq] Prediction: 1, Confidence: 0.8037\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto0/pluto0_2m_adv1m_noise.iq\n",
      "[pluto0_2m_adv1m_noise.iq] Prediction: 1, Confidence: 0.8519\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto10/pluto10_2m_adv1m_concateotlow.iq\n",
      "[pluto10_2m_adv1m_concateotlow.iq] Prediction: 0, Confidence: 1.0000\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto10/pluto10_2m_adv1m_eotlow.iq\n",
      "[pluto10_2m_adv1m_eotlow.iq] Prediction: 2, Confidence: 0.9848\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto10/pluto10_2m_adv1m_eotmid.iq\n",
      "[pluto10_2m_adv1m_eotmid.iq] Prediction: 2, Confidence: 1.0000\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto10/pluto10_2m_adv1m_noise.iq\n",
      "[pluto10_2m_adv1m_noise.iq] Prediction: 2, Confidence: 0.9934\n",
      "\n",
      "Processing file: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11/pluto10/pluto10_2m_adv1m_concateotmid.iq\n",
      "[pluto10_2m_adv1m_concateotmid.iq] Prediction: 2, Confidence: 0.9372\n"
     ]
    }
   ],
   "source": [
    "TRUE_IQ_FILE_PATH = \"Apr9/mid_dist_eot_pluto10_to_target1_noise.iq\"\n",
    "WINDOW_SIZE = 10000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model\n",
    "model = resnet50_1d(num_classes=8)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "BASE_DIR = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/justin/Apr11\"\n",
    "TARGET_DIRS = [\"pluto0\", \"pluto10\"]\n",
    "iq_file_paths = []\n",
    "\n",
    "for subdir in TARGET_DIRS:\n",
    "    dir_path = os.path.join(BASE_DIR, subdir)\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".iq\"):\n",
    "            iq_file_paths.append(os.path.join(dir_path, file))\n",
    "\n",
    "# Loop through each file and test\n",
    "for file_path in iq_file_paths:\n",
    "    fname = os.path.basename(file_path)\n",
    "    print(f\"\\nProcessing file: {file_path}\")\n",
    "\n",
    "    # Assign label based on directory (adjust if your labels differ)\n",
    "    if \"pluto0\" in file_path:\n",
    "        target_label = 0\n",
    "    elif \"pluto10\" in file_path:\n",
    "        target_label = 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label source in path: {file_path}\")\n",
    "\n",
    "    # Load IQ data\n",
    "    data = np.fromfile(file_path, dtype=\"float32\")\n",
    "    real_part = data[0::2]\n",
    "    imag_part = data[1::2]\n",
    "\n",
    "    # Check file length\n",
    "    start = 0\n",
    "    end = start + WINDOW_SIZE\n",
    "    if end > len(real_part):\n",
    "        print(f\"[{fname}] Skipped: Not enough samples (required {end}, got {len(real_part)})\")\n",
    "        continue\n",
    "\n",
    "    # Extract 1 window\n",
    "    i_window = real_part[start:end]\n",
    "    q_window = imag_part[start:end]\n",
    "    combined = np.vstack((i_window, q_window))  # shape: [2, WINDOW_SIZE]\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    test_dataset = IQDataset([combined], [target_label])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            pred = torch.argmax(probs, dim=1)\n",
    "            conf = probs.max(dim=1).values\n",
    "            print(f\"[{fname}] Prediction: {pred.item()}, Confidence: {conf.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1238695-3b44-405f-8286-1b5183d148fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_inverse_channel_compensation(x, target_distance=1.5, path_loss_exponent=2.0, reference_distance=1.0, apply_inverse_fading=True):\n",
    "\n",
    "    # calculate scaling power like before\n",
    "    path_loss_scaling = (reference_distance / target_distance) ** (path_loss_exponent / 2)\n",
    "    x_compensated = x / path_loss_scaling  # INVERSE of attenuation\n",
    "\n",
    "    # Estimate and invert fading\n",
    "    if apply_inverse_fading:\n",
    "        # Generate a sample fading coefficient (complex Rayleigh)\n",
    "        fading = (torch.randn_like(x) + 1j * torch.randn_like(x)) / np.sqrt(2)\n",
    "        x_compensated = x_compensated / fading  # INVERSE of fading\n",
    "\n",
    "    return x_compensated\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
