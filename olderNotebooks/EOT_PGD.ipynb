{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc49258-6479-4781-86b9-6ab81e326c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F  \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833b00b2-77e0-423c-a85f-afecf5c2e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.05       # Max perturbation (for L∞ PGD)\n",
    "ALPHA = 0.01         # Step size per iteration\n",
    "ATTACK_ITERATIONS = 20\n",
    "TARGET_LABEL = 3     # Example target label for the targeted attack\n",
    "EOT_ITERATIONS = 5\n",
    "# System/Model parameters\n",
    "sys.path.append(\"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/models\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#MODEL_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/weights/justin_model_slicing_norm_LR1_smallHop_12_largerInput.pth\"\n",
    "MODEL_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/weights/best_model.pth\"\n",
    "#IQ_FILE_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/1m_2m_replacedPluto4/1m_2m/Pluto_0_1m_run0.iq\"  # Example file\n",
    "IQ_FILE_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/1m_2m_replacedPluto4/1m_2m/Pluto_0_2m_run0.iq\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9854e10e-531a-483c-a6f8-2872c72c17e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model weights from: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/weights/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3712893/379228525.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet1D(\n",
       "  (conv1): Conv1d(2, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck1D(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck1D(\n",
       "      (conv1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Bottleneck1D(\n",
       "      (conv1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck1D(\n",
       "      (conv1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck1D(\n",
       "      (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Bottleneck1D(\n",
       "      (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Bottleneck1D(\n",
       "      (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck1D(\n",
       "      (conv1): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck1D(\n",
       "      (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(1024, 2048, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck1D(\n",
       "      (conv1): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Bottleneck1D(\n",
       "      (conv1): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=2048, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from attempt2 import resnet50_1d  # Directly import from attempt2.py\n",
    "num_classes = 8  # Change this if your model was trained with a different number of classes\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = resnet50_1d(num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "# Load trained weights\n",
    "#MODEL_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/weights/justin_model_slicing_norm_LR1_smallHop_12.pth\"\n",
    "print(f\"Loading trained model weights from: {MODEL_PATH}\")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b45a605-bf86-4470-a73e-27ac1dc36150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iq_data(file_path, max_samples=4096*2, start_idx=0):\n",
    "    \"\"\"\n",
    "    Loads a limited portion of `.iq` data.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the `.iq` file.\n",
    "        max_samples (int): The number of samples to load.\n",
    "        start_idx (int): The starting index to read from.\n",
    "\n",
    "    Returns:\n",
    "        data_tensor (torch.Tensor): shape [1, 2, max_samples]\n",
    "        label_tensor (torch.Tensor): Dummy label for testing.\n",
    "    \"\"\"\n",
    "    total_samples = max_samples * 2  # Since I/Q samples are interleaved\n",
    "\n",
    "    #  Open the file in binary mode and seek to `start_idx`\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        f.seek(start_idx * 4 * 2)  # 4 bytes per float32, 2 channels (I/Q)\n",
    "        raw_data = np.fromfile(f, dtype=\"float32\", count=total_samples)\n",
    "\n",
    "    #  Ensure we have enough data\n",
    "    if raw_data.shape[0] < total_samples:\n",
    "        raise ValueError(f\"Not enough data in {file_path}. Requested {total_samples}, got {raw_data.shape[0]}.\")\n",
    "\n",
    "    #  Extract I/Q channels\n",
    "    I = raw_data[0::2]  # Even indices\n",
    "    Q = raw_data[1::2]  # Odd indices\n",
    "\n",
    "    #  Stack into [2, max_samples] format\n",
    "    iq_data = np.stack([I, Q], axis=0)\n",
    "\n",
    "    #  Add batch dimension → [1, 2, max_samples]\n",
    "    iq_data = np.expand_dims(iq_data, axis=0)\n",
    "\n",
    "    #  Convert to PyTorch tensor\n",
    "    data_tensor = torch.from_numpy(iq_data).float()\n",
    "\n",
    "    #  Dummy label for now (adjust if needed)\n",
    "    label_tensor = torch.tensor([0], dtype=torch.long)\n",
    "\n",
    "    return data_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a8f44c-0bf1-442c-b617-f27678463b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_distance_path_loss(x, min_distance=5.0, max_distance=7.0, path_loss_exponent=2.0, reference_distance=1.0):\n",
    "    #Sample a random dist between 1 and 2 m \n",
    "    d = torch.empty(1, device=x.device).uniform_(min_distance, max_distance)\n",
    "    #Compute the path loss scaling\n",
    "    scaling = (reference_distance / d) ** path_loss_exponent\n",
    "    return x * scaling\n",
    "\n",
    "\n",
    "def transform_channel_effects(x, min_distance=1.0, max_distance=2.0, path_loss_exponent=2.0, reference_distance=1.0, noise_std=0.01, apply_fading=True, apply_phase=True):\n",
    "    # Sample a random distance between min and max.\n",
    "    d = torch.empty(1, device=x.device).uniform_(min_distance, max_distance)\n",
    "    \n",
    "    # Compute path loss scaling.\n",
    "    scaling = (reference_distance / d) ** path_loss_exponent\n",
    "    x_transformed = x * scaling\n",
    "\n",
    "    # Apply fading (e.g., Rayleigh fading)\n",
    "    if apply_fading:\n",
    "        # Generate Rayleigh fading factor: sqrt(X^2 + Y^2) for two independent gaussians.\n",
    "        fading = torch.sqrt(torch.randn_like(x_transformed)**2 + torch.randn_like(x_transformed)**2)\n",
    "        x_transformed = x_transformed * fading\n",
    "\n",
    "    # Add AWGN noise.\n",
    "    noise = torch.randn_like(x_transformed) * noise_std\n",
    "    x_transformed = x_transformed + noise\n",
    "\n",
    "    # Apply phase rotation (for IQ signals, if represented in a complex manner or separate I/Q channels).\n",
    "    if apply_phase:\n",
    "        # For simplicity, assume x is a real tensor with I and Q channels concatenated.\n",
    "        # You might want to construct a complex representation. Here we apply the same rotation on both channels.\n",
    "        phase = torch.empty(1, device=x.device).uniform_(0, 2 * np.pi)\n",
    "        x_transformed = x_transformed * torch.cos(phase)  # simple approximation for phase rotation\n",
    "\n",
    "    return x_transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7067270c-03ec-40f2-b35c-e880758d8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targeted_eot_pgd_attack(model, x, y, target_label, eps, alpha, num_iter, eot_iter):\n",
    "    \"\"\"\n",
    "    Targeted PGD attack with Expectation Over Transformation (EOT) using a distance-based path loss transformation.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to attack.\n",
    "        x (torch.Tensor): Original input tensor.\n",
    "        y (torch.Tensor): True labels.\n",
    "        target_label (int or torch.Tensor): The target class for the attack.\n",
    "        eps (float): Maximum perturbation.\n",
    "        alpha (float): Step size.\n",
    "        num_iter (int): Number of PGD iterations.\n",
    "        eot_iter (int): Number of EOT iterations per PGD step.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Adversarial examples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(target_label, int):\n",
    "        target_label = torch.full_like(y, target_label)\n",
    "    \n",
    "    x_adv = x.clone().detach().to(x.device)\n",
    "    x_adv.requires_grad = True\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        grad_accum = torch.zeros_like(x_adv)\n",
    "        \n",
    "        # Run multiple transformations for EOT and accumulate gradients.\n",
    "        for _ in range(eot_iter):\n",
    "            # Clone the current adversarial input and set requires_grad.\n",
    "            new_inputs = x_adv.detach().clone().requires_grad_()\n",
    "            transformed = transform_channel_effects(new_inputs)\n",
    "            \n",
    "            outputs = model(transformed)\n",
    "            # For a targeted attack, minimize negative loss to push predictions toward target_label.\n",
    "            loss = -loss_fn(outputs, target_label)\n",
    "            \n",
    "            # Compute gradient with respect to new_inputs.\n",
    "            grad = torch.autograd.grad(loss, new_inputs)[0]\n",
    "            grad_accum += grad\n",
    "        \n",
    "        # Average the accumulated gradients.\n",
    "        grad_avg = grad_accum / eot_iter\n",
    "        \n",
    "        # Update adversarial input with the sign of the averaged gradient.\n",
    "        x_adv = x_adv.detach() + alpha * grad_avg.sign()\n",
    "        \n",
    "        # Project the perturbation back to the epsilon ball and clip to valid range.\n",
    "        x_adv = torch.min(torch.max(x_adv, x - eps), x + eps)\n",
    "        x_adv = torch.clamp(x_adv, 0, 1)\n",
    "        x_adv.requires_grad = True\n",
    "\n",
    "    return x_adv.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a96ba8fe-7e31-4ffc-b7f0-602109f7776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/1m_2m_replacedPluto4/1m_2m/Pluto_0_2m_run0.iq\n",
      "Original data shape:  torch.Size([1, 2, 8192])\n",
      "Original prediction: [5], Confidence: 0.9728\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(f\"Loading data from: {IQ_FILE_PATH}\")\n",
    "data, labels = load_iq_data(IQ_FILE_PATH)\n",
    "data, labels = data.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "print(\"Original data shape: \", data.shape)  # Expected shape: [1, 2, N]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(data)  # Raw model output\n",
    "    probs = F.softmax(logits, dim=1)  # Convert to probabilities\n",
    "    original_pred = torch.argmax(probs, dim=1)  # Get predicted class\n",
    "    confidence = probs.max(dim=1).values  # Get confidence score of predicted class\n",
    "\n",
    "print(f\"Original prediction: {original_pred.cpu().numpy()}, Confidence: {confidence.cpu().item():.4f}\")\n",
    "\n",
    "# Make adversarial example with targeted EOT PGD attack\n",
    "x_adv = targeted_eot_pgd_attack(\n",
    "    model=model,\n",
    "    x=data,\n",
    "    y=labels,\n",
    "    target_label=TARGET_LABEL,  \n",
    "    eps=EPSILON,\n",
    "    alpha=ALPHA,\n",
    "    num_iter=ATTACK_ITERATIONS,\n",
    "    eot_iter=EOT_ITERATIONS\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba1b8ac2-d4a7-4b7e-bb04-2be0d067cf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 values of the interleaved perturbation sequence: [-5.4933305e-04 -1.1291846e-03  2.9999999e-02  2.9999999e-02\n",
      " -6.1037004e-05 -7.5669959e-10  5.0000001e-02  4.9694814e-02\n",
      "  3.8962372e-02  3.0000001e-02]\n",
      "Adversarial prediction: [3], Confidence: 0.9998\n",
      "Confidence for class 3: 0.9998\n",
      "First perturbation value (I channel): -0.00054933305\n"
     ]
    }
   ],
   "source": [
    "\n",
    "perturbations = x_adv - data  # Get the difference between adversarial and original data\n",
    "perturbations_np = perturbations.cpu().detach().numpy()\n",
    "\n",
    "data_np = data.cpu().detach().numpy()\n",
    "x_adv_np = x_adv.cpu().detach().numpy()\n",
    "\n",
    "I_orig = data_np[0, 0, :]  # Original In-phase component\n",
    "Q_orig = data_np[0, 1, :]  # Original Quadrature component\n",
    "\n",
    "I_adv = x_adv_np[0, 0, :]  # Adversarial In-phase component\n",
    "Q_adv = x_adv_np[0, 1, :]  # Adversarial Quadrature component\n",
    "\n",
    "I_diff = perturbations_np[0, 0, :]  # Perturbation (I channel)\n",
    "Q_diff = perturbations_np[0, 1, :]  # Perturbation (Q channel)\n",
    "\n",
    "interleaved_perturbations = np.empty((I_diff.size + Q_diff.size,), dtype=np.float32)\n",
    "interleaved_perturbations[0::2] = I_diff  # Place I values at even indices\n",
    "interleaved_perturbations[1::2] = Q_diff  # Place Q values at odd indices\n",
    "\n",
    "# Save the interleaved perturbation IQ sequence as a binary .iq file\n",
    "FILENAME = \"2m_0target3_noise.iq\"\n",
    "interleaved_perturbations.astype(\"float32\").tofile(FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# Interleave I and Q to form an IQIQIQ... sequence\n",
    "interleaved = np.empty((I_diff.size + Q_diff.size,), dtype=I_diff.dtype)\n",
    "interleaved[0::2] = I_diff\n",
    "interleaved[1::2] = Q_diff\n",
    "\n",
    "print(\"First 10 values of the interleaved perturbation sequence:\", interleaved[:10])\n",
    "np.savetxt(\"perturbations_sequence.txt\", interleaved, fmt='%f')\n",
    "\n",
    "# Evaluate the adversarial example\n",
    "with torch.no_grad():\n",
    "    adv_logits = model(x_adv)  # Raw output for adversarial example\n",
    "    adv_probs = F.softmax(adv_logits, dim=1)  # Convert to probabilities\n",
    "    adv_pred = torch.argmax(adv_probs, dim=1)  # Get predicted class\n",
    "    adv_confidence = adv_probs.max(dim=1).values  # Get confidence score of predicted class\n",
    "    \n",
    "    # Print out confidence for class 3 regardless of prediction.\n",
    "    # Assuming a single example (batch size = 1)\n",
    "    confidence_class3 = adv_probs[0, 3].item()\n",
    "\n",
    "print(f\"Adversarial prediction: {adv_pred.cpu().numpy()}, Confidence: {adv_confidence.cpu().item():.4f}\")\n",
    "print(f\"Confidence for class 3: {confidence_class3:.4f}\")\n",
    "print(\"First perturbation value (I channel):\", I_diff[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f696a53-1f0b-49d6-b095-55c80b2f4e88",
   "metadata": {},
   "source": [
    "Here below is the EOT PGD that uses transforms_channel_effects to simulate realistic channel conditions over distances from min = 5 to max = 7 meters\n",
    "\n",
    "I have a random start in the targeted eot pgd attack to introduce randomness in starting point\n",
    "\n",
    "For each pgd step the code performs eot integrations transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e674626c-0c2d-4c2f-b5c9-e591cc02a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def transform_distance_path_loss(x, min_distance=5.0, max_distance=7.0, path_loss_exponent=2.0, reference_distance=1.0):\n",
    "    # Sample a random distance between min_distance and max_distance (e.g., 5 to 7 m)\n",
    "    d = torch.empty(1, device=x.device).uniform_(min_distance, max_distance)\n",
    "    # Compute the path loss scaling factor\n",
    "    scaling = (reference_distance / d) ** path_loss_exponent\n",
    "    return x * scaling\n",
    "\n",
    "def transform_channel_effects(x, min_distance=5.0, max_distance=7.0, path_loss_exponent=2.0, reference_distance=1.0, noise_std=0.01, apply_fading=True, apply_phase=True):\n",
    "    # Sample a random distance between min_distance and max_distance\n",
    "    d = torch.empty(1, device=x.device).uniform_(min_distance, max_distance)\n",
    "    \n",
    "    # Compute path loss scaling\n",
    "    scaling = (reference_distance / d) ** path_loss_exponent\n",
    "    x_transformed = x * scaling\n",
    "\n",
    "    # Optionally apply fading (e.g., Rayleigh fading)\n",
    "    if apply_fading:\n",
    "        fading = torch.sqrt(torch.randn_like(x_transformed)**2 + torch.randn_like(x_transformed)**2)\n",
    "        x_transformed = x_transformed * fading\n",
    "\n",
    "    # Add AWGN noise\n",
    "    noise = torch.randn_like(x_transformed) * noise_std\n",
    "    x_transformed = x_transformed + noise\n",
    "\n",
    "    # Optionally apply phase rotation (for IQ signals)\n",
    "    if apply_phase:\n",
    "        phase = torch.empty(1, device=x.device).uniform_(0, 2 * np.pi)\n",
    "        x_transformed = x_transformed * torch.cos(phase)\n",
    "    \n",
    "    return x_transformed\n",
    "\n",
    "def targeted_eot_pgd_attack(model, x, y, target_label, eps, alpha, num_iter, eot_iter, use_random_start=True):\n",
    "    \"\"\"\n",
    "    Targeted PGD attack with Expectation Over Transformation (EOT) that applies\n",
    "    a random transformation (channel effects and/or distance path loss) every EOT iteration.\n",
    "    \n",
    "    The idea is that in every PGD step, we generate multiple transformed versions\n",
    "    of the adversarial sample (simulating different distances, channel conditions, etc.),\n",
    "    average the computed gradients, and update the adversarial example accordingly.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(target_label, int):\n",
    "        target_label = torch.full_like(y, target_label)\n",
    "    \n",
    "    x_adv = x.clone().detach().to(x.device)\n",
    "    \n",
    "    if use_random_start:\n",
    "        x_adv = x_adv + torch.empty_like(x_adv).uniform_(-eps, eps)\n",
    "        x_adv = torch.clamp(x_adv, 0, 1).detach()\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for step in range(num_iter):\n",
    "        grad_accum = torch.zeros_like(x_adv)\n",
    "        x_adv.requires_grad = True\n",
    "\n",
    "        for _ in range(eot_iter):\n",
    "            new_inputs = x_adv.detach().clone().requires_grad_()\n",
    "            transformed = transform_channel_effects(new_inputs, min_distance=5.0, max_distance=7.0)\n",
    "\n",
    "            transformed = F.adaptive_avg_pool1d(transformed, 8192)\n",
    "            \n",
    "            outputs = model(transformed)\n",
    "        \n",
    "            loss = -loss_fn(outputs, target_label)\n",
    "            grad = torch.autograd.grad(loss, new_inputs)[0]\n",
    "            grad_accum += grad\n",
    "\n",
    "        # Average the accumulated gradients over EOT iterations\n",
    "        grad_avg = grad_accum / eot_iter\n",
    "\n",
    "        # Update adversarial input using the sign of the averaged gradient.\n",
    "        x_adv = x_adv.detach() + alpha * grad_avg.sign()\n",
    "\n",
    "        # Project the updated adversarial sample back into the epsilon ball and valid range [0, 1]\n",
    "        x_adv = torch.min(torch.max(x_adv, x - eps), x + eps)\n",
    "        x_adv = torch.clamp(x_adv, 0, 1).detach()\n",
    "\n",
    "    return x_adv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aa06a67-0bd3-497d-a8cd-34ab0010934d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 values of the interleaved perturbation sequence: [ 0.01568628  0.00671395 -0.00073244  0.01516746  0.00784314  0.01570334\n",
      "  0.00680551  0.00753795 -0.00103763  0.02352941]\n",
      "Adversarial prediction: [3], Confidence: 0.7959\n",
      "Confidence for class 3: 0.7959\n",
      "First perturbation value (I channel): 0.015686275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eps = 8 / 255\n",
    "alpha = 2 / 255\n",
    "num_iter = 10\n",
    "eot_iter = 2\n",
    "target_class = 3  # example target label\n",
    "\n",
    "# Generate adversarial examples using the targeted EOT PGD attack\n",
    "x_adv = targeted_eot_pgd_attack(model, data, labels, target_class, eps, alpha, num_iter, eot_iter)\n",
    "\n",
    "# Calculate the perturbations (difference between adversarial and original data)\n",
    "perturbations = x_adv - data  \n",
    "perturbations_np = perturbations.cpu().detach().numpy()\n",
    "\n",
    "data_np = data.cpu().detach().numpy()\n",
    "x_adv_np = x_adv.cpu().detach().numpy()\n",
    "\n",
    "# Assume IQ samples have two channels: In-phase (I) and Quadrature (Q)\n",
    "I_orig = data_np[0, 0, :]  # Original I channel\n",
    "Q_orig = data_np[0, 1, :]  # Original Q channel\n",
    "\n",
    "I_adv = x_adv_np[0, 0, :]  # Adversarial I channel\n",
    "Q_adv = x_adv_np[0, 1, :]  # Adversarial Q channel\n",
    "\n",
    "I_diff = perturbations_np[0, 0, :]  # Perturbation on I channel\n",
    "Q_diff = perturbations_np[0, 1, :]  # Perturbation on Q channel\n",
    "\n",
    "# Interleave I and Q perturbations (I0, Q0, I1, Q1, ...)\n",
    "interleaved_perturbations = np.empty((I_diff.size + Q_diff.size,), dtype=np.float32)\n",
    "interleaved_perturbations[0::2] = I_diff\n",
    "interleaved_perturbations[1::2] = Q_diff\n",
    "\n",
    "# Save the interleaved perturbation IQ sequence as a binary .iq file\n",
    "FILENAME = \"2m_0target3_noise.iq\"\n",
    "interleaved_perturbations.astype(\"float32\").tofile(FILENAME)\n",
    "\n",
    "# Also save the perturbation sequence to a text file for inspection\n",
    "np.savetxt(\"perturbations_sequence.txt\", interleaved_perturbations, fmt='%f')\n",
    "print(\"First 10 values of the interleaved perturbation sequence:\", interleaved_perturbations[:10])\n",
    "\n",
    "# Evaluate the adversarial example by checking model predictions\n",
    "with torch.no_grad():\n",
    "    # Apply the same pooling to x_adv before evaluation to match the input size expected by the model.\n",
    "    x_adv_evaluated = F.adaptive_avg_pool1d(x_adv, 8192)\n",
    "    adv_logits = model(x_adv_evaluated)\n",
    "    adv_probs = F.softmax(adv_logits, dim=1)\n",
    "    adv_pred = torch.argmax(adv_probs, dim=1)\n",
    "    adv_confidence = adv_probs.max(dim=1).values\n",
    "    \n",
    "    # For demonstration, print confidence for class 3\n",
    "    confidence_class3 = adv_probs[0, 3].item()\n",
    "\n",
    "print(f\"Adversarial prediction: {adv_pred.cpu().numpy()}, Confidence: {adv_confidence.cpu().item():.4f}\")\n",
    "print(f\"Confidence for class 3: {confidence_class3:.4f}\")\n",
    "print(\"First perturbation value (I channel):\", I_diff[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb23da-48b9-49c7-9171-7a4f2ed6956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "2m_0target3_pathloss.iq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
