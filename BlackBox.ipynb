{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3baddb0b-ec45-4c68-8c8a-68e169e87854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model weights from: /home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/weights/best_model_retrained.pth\n",
      "model is loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1091547/1990433863.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F  \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EPSILON = 0.1       # Max perturbation (for Lâˆž PGD)\n",
    "ALPHA = 0.01         # Step size per iteration\n",
    "ATTACK_ITERATIONS = 40\n",
    "TARGET_LABEL = 2     # Example target label for the targeted attack\n",
    "\n",
    "# System/Model parameters\n",
    "sys.path.append(\"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/models\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/weights/best_model_retrained.pth\"\n",
    "#IQ_FILE_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/1m_2m_replacedPluto4/Pluto_10_windows_runs2_3/Pluto_10_2m_run3.iq\"\n",
    "IQ_FILE_PATH = \"/home/jfeng/Desktop/jfeng/rf_spoofing/spoofing/1m_2m_replacedPluto4/Pluto_10_windows_runs2_3/Pluto_10_2m_run3.iq\"\n",
    "\n",
    "from attempt2 import resnet50_1d  # Directly import from attempt2.py\n",
    "num_classes = 8  # Change this if your model was trained with a different number of classes\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = resnet50_1d(num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "# Load trained weights\n",
    "print(f\"Loading trained model weights from: {MODEL_PATH}\")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "print(\"model is loaded\")\n",
    "class IQDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        sample = torch.from_numpy(sample).float()\n",
    "        # Normalize data\n",
    "        magnitude = torch.sqrt(torch.sum(sample**2, dim=1, keepdim=True))\n",
    "        sample = sample / magnitude\n",
    "\n",
    "        label_tensors = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return sample, label_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62635f0a-01c2-4095-a27d-d867efd0e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "WINDOW_SIZE = 10000\n",
    "HOP_SIZE = 500\n",
    "START_INDEX = 4800\n",
    "END_INDEX = 6000\n",
    "\n",
    "def path_loss(signal, n, d1, d2):\n",
    "    scaling = (d1 / d2) ** (n / 2)\n",
    "    return signal * scaling\n",
    "\n",
    "def apply_rician_fading(signal, K=10.0):\n",
    "    \"\"\"\n",
    "    Apply Rician fading to the signal, which is of shape [2, N] (I and Q).\n",
    "    Fading is applied using a K-factor (default 10).\n",
    "    \"\"\"\n",
    "    i_window = signal[:, 0, :]\n",
    "    q_window = signal[:, 1, :]\n",
    "\n",
    "    device = signal.device\n",
    "    dtype = signal.dtype\n",
    "    # Calculate scaling factors\n",
    "    K = torch.tensor(K, dtype=dtype, device=device)\n",
    "    scale_LOS = torch.sqrt(K / (K + 1))\n",
    "    scale_NLOS = torch.sqrt(1 / (K + 1))\n",
    "\n",
    "    # Generate NLOS (Rayleigh) component\n",
    "    real_nlos = torch.randn(i_window.shape, device=device) / torch.sqrt(torch.tensor(2.0, dtype=dtype, device=device))\n",
    "    imag_nlos = torch.randn(i_window.shape, device=device) / torch.sqrt(torch.tensor(2.0, dtype=dtype, device=device))\n",
    "\n",
    "    # LOS component (typically assumed as 1 + 0j for all samples)\n",
    "    real_los = torch.ones(i_window.shape, device=device)\n",
    "    imag_los = torch.zeros(i_window.shape, device=device)\n",
    "\n",
    "    # Total fading coefficients\n",
    "    real_fade = scale_LOS * real_los + scale_NLOS * real_nlos\n",
    "    imag_fade = scale_LOS * imag_los + scale_NLOS * imag_nlos\n",
    "\n",
    "    # Apply Rician fading\n",
    "    faded_real = i_window * real_fade - q_window * imag_fade\n",
    "    faded_imag = i_window * imag_fade + q_window * real_fade\n",
    "\n",
    "    # Reconstruct the faded signal back into a tensor\n",
    "    faded_signal = torch.stack((faded_real, faded_imag), dim=1)\n",
    "\n",
    "    return faded_signal\n",
    "\n",
    "def apply_awgn(signal, noise_std=0.000001):\n",
    "    device = signal.device\n",
    "    dtype = signal.dtype\n",
    "    # Split noise equally between I and Q (to maintain total variance)\n",
    "    per_dim_std = noise_std / torch.sqrt(torch.tensor(2.0, dtype=dtype, device=device))\n",
    "    # Generate i.i.d. Gaussian noise for I and Q\n",
    "    noise = torch.randn_like(signal, device=device) * per_dim_std\n",
    "    # Add noise to the signal\n",
    "    noisy_signal = signal + noise\n",
    "\n",
    "    return noisy_signal\n",
    "\n",
    "def transform_channel_effects(x, chosen_distance=2.0, path_loss_exponent=2.0, reference_distance=1.0, noise_std=0.000001, k=10.0):   \n",
    "    #print(\"Signal original: \", x)\n",
    "    signal_path_loss = path_loss(x, path_loss_exponent, reference_distance, chosen_distance)\n",
    "    #print(\"Signal Path Loss: \", signal_path_loss)\n",
    "    signal_rician = apply_rician_fading(signal_path_loss, k)\n",
    "    #print(\"Signal Rician: \", signal_rician)\n",
    "    signal_awgn = apply_awgn(signal_rician, noise_std)\n",
    "    #print(\"Signal AWGN: \", signal_awgn)\n",
    "    return signal_awgn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4c3e6-f0d6-4753-b590-307ae0a62476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_limited_nes_attack( # most of the params are unchanged from the previous attacks\n",
    "    model,\n",
    "    x,\n",
    "    perturb,\n",
    "    y,\n",
    "    target_label,\n",
    "    eps=0.1,\n",
    "    alpha=0.01,\n",
    "    num_iter=40,\n",
    "    num_queries=20,     # Number of queries for NES black box limiting\n",
    "    sigma=0.01,         # Small perturbation scale\n",
    "    num_samples=10,     # EOT samples (for realism, can be smaller)\n",
    "    min_distance=1.0,\n",
    "    max_distance=4.0,\n",
    "    path_loss_exponent=2.0,\n",
    "    reference_distance=1.0,\n",
    "    min_noise_std=0.000001,\n",
    "    max_noise_std=0.0001,\n",
    "    min_k=10,\n",
    "    max_k=20\n",
    "):\n",
    "    x_adv = perturb.clone().detach().to(DEVICE) \n",
    "    target = torch.full_like(y, target_label).to(DEVICE)\n",
    "\n",
    "    best_x_adv = x_adv.clone().detach()\n",
    "    best_target_confidence = -float('inf')\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        nes_grad_estimate = torch.zeros_like(x_adv)\n",
    "\n",
    "        # NES Gradient Estimation via queries\n",
    "        for _ in range(num_queries): # only prompt in the range of inputted num_queries size\n",
    "            u = torch.randn_like(x_adv) # small perterbation drawn \n",
    "            \n",
    "            # Evaluate model at (x_adv + sigma*u) using EOT for realism\n",
    "            logits_plus = 0\n",
    "            for _ in range(num_samples):\n",
    "                chosen_distance = torch.empty(1).uniform_(min_distance, max_distance).item()\n",
    "                chosen_k = torch.empty(1).uniform_(min_k, max_k).item()\n",
    "                chosen_noise_std = torch.empty(1).uniform_(min_noise_std, max_noise_std).item()\n",
    "                \n",
    "                x_t_plus = transform_channel_effects(\n",
    "                    x_adv + sigma * u,\n",
    "                    chosen_distance,\n",
    "                    path_loss_exponent,\n",
    "                    reference_distance,\n",
    "                    chosen_noise_std,\n",
    "                    chosen_k\n",
    "                ) + x\n",
    "\n",
    "                logits_plus += model(x_t_plus)\n",
    "            \n",
    "            logits_plus /= num_samples #logits_plus is avg model output (logits) across transformations\n",
    "            loss_plus = F.cross_entropy(logits_plus, target)\n",
    "\n",
    "            nes_grad_estimate += loss_plus.item() * u\n",
    "\n",
    "        nes_grad_estimate /= (sigma * num_queries)\n",
    "\n",
    "        # Update adversarial example using NES gradient approximation\n",
    "        with torch.no_grad():\n",
    "            x_adv -= alpha * nes_grad_estimate.sign()\n",
    "            x_adv = torch.max(torch.min(x_adv, x + eps), x - eps)\n",
    "\n",
    "        # Evaluate and track best adversarial example\n",
    "        with torch.no_grad():\n",
    "            logits_eval = 0\n",
    "            for _ in range(num_samples):\n",
    "                chosen_distance = torch.empty(1).uniform_(min_distance, max_distance).item()\n",
    "                chosen_k = torch.empty(1).uniform_(min_k, max_k).item()\n",
    "                chosen_noise_std = torch.empty(1).uniform_(min_noise_std, max_noise_std).item()\n",
    "\n",
    "                x_t_eval = transform_channel_effects(\n",
    "                    x_adv,\n",
    "                    chosen_distance,\n",
    "                    path_loss_exponent,\n",
    "                    reference_distance,\n",
    "                    chosen_noise_std,\n",
    "                    chosen_k\n",
    "                ) + x\n",
    "\n",
    "                logits_eval += model(x_t_eval)\n",
    "\n",
    "            logits_eval /= num_samples\n",
    "            avg_confidence = F.softmax(logits_eval, dim=1)[0, target_label].item()\n",
    "\n",
    "            if avg_confidence > best_target_confidence:\n",
    "                best_target_confidence = avg_confidence\n",
    "                best_x_adv = x_adv.clone().detach()\n",
    "\n",
    "            pred = logits_eval.argmax(dim=1)\n",
    "            success = (pred == target_label).float().mean().item()\n",
    "            print(f\"Iteration {i+1}/{num_iter}: Attack success = {success*100:.2f}%\")\n",
    "\n",
    "    return best_x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd68ace-2c40-4f00-97aa-2fd868a2c6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2804f-ba9e-4cc2-a256-9144fa4c01b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
